{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: 验证probe翻译的棋盘与groundtruth一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(44)\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from data import get_othello, plot_probs, plot_mentals\n",
    "from data.othello import permit, start_hands, OthelloBoardState, permit_reverse\n",
    "from mingpt.dataset import CharDataset\n",
    "from mingpt.model import GPT, GPTConfig, GPTforProbeIA\n",
    "from mingpt.utils import sample, intervene, print_board\n",
    "from mingpt.probe_model import BatteryProbeClassification, BatteryProbeClassificationTwoLayer\n",
    "championship = False\n",
    "mid_dim = 128\n",
    "how_many_history_step_to_use = 99\n",
    "exp = f\"state_tl{mid_dim}\"\n",
    "if championship:\n",
    "    exp += \"_championship\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load benchmark dataset and its samples\n",
    "(verification的相同samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intervention_benchmark.pkl\", \"rb\") as input_file:\n",
    "    dataset = pickle.load(input_file)\n",
    "\n",
    "case_ids = [0, 16, 99, 123, 233, 500, 666, 777, 888, 1000]\n",
    "completions = [dataset[case_id][\"history\"] for case_id in case_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intervention_benchmark.pkl\", \"rb\") as input_file:\n",
    "    dataset = pickle.load(input_file)\n",
    "\n",
    "completions = [data[\"history\"] for data in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load non-linear probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-07030adcae1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlayer_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_e\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatteryProbeClassificationTwoLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobe_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmid_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmid_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mload_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./ckpts/battery_othello/{exp}/layer{layer}/checkpoint.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# state_tl128/layer[4:9]/checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Set the module in evaluation mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/othello/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0;34mr\"\"\"Return the index of a currently selected device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/othello/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "probes = {}\n",
    "layer_s = 1\n",
    "layer_e = 9\n",
    "for layer in range(layer_s, layer_e):\n",
    "    p = BatteryProbeClassificationTwoLayer(torch.cuda.current_device(), probe_class=3, num_task=64, mid_dim=mid_dim)\n",
    "    load_res = p.load_state_dict(torch.load(f\"./ckpts/battery_othello/{exp}/layer{layer}/checkpoint.ckpt\")) # state_tl128/layer[4:9]/checkpoint\n",
    "    p.eval() # Set the module in evaluation mode.\n",
    "    probes[layer] = p # probes用来装(layer数量, probe) pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# othello = get_othello(ood_perc=.2, data_root=\"data/othello_pgn\", wthor=False)\n",
    "othello = get_othello(ood_perc=0., data_root=None, wthor=False, ood_num=1)\n",
    "train_dataset = CharDataset(othello)\n",
    "\n",
    "mconf = GPTConfig(61, 59, n_layer=8, n_head=8, n_embd=512)\n",
    "\n",
    "models = {}\n",
    "for layer in range(layer_s, layer_e):\n",
    "    model = GPTforProbeIA(mconf, probe_layer=layer)\n",
    "    # model = GPT(mconf)\n",
    "    load_res = model.load_state_dict(torch.load(\"./ckpts/gpt_synthetic.ckpt\" if not championship else \"./ckpts/gpt_championship.ckpt\"))\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.cuda.current_device()\n",
    "        model = model.to(device)\n",
    "    _ = model.eval()\n",
    "    models[layer] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare the board labels between the probe and the model for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_rate = 0\n",
    "for j, completion in enumerate(completions):\n",
    "    probe_trans = []\n",
    "    partial_game = torch.tensor([train_dataset.stoi[s] for s in completion], dtype=torch.long).to(device) \n",
    "    '''get ground truth'''\n",
    "    ab = OthelloBoardState()\n",
    "    ab.update(completion, prt=False)\n",
    "    pre_intv_truth = ab.get_state()\n",
    "    '''1st layer'''\n",
    "    p = probes[layer_s]\n",
    "    whole_mid_act = models[layer_s].forward_1st_stage(partial_game[None, :])\n",
    "    mid_act = whole_mid_act[0, -1] #[512, ], first batch, last token\n",
    "    pre_intv_logits = p(mid_act[None, :])[0].squeeze(0)     \n",
    "    labels_pre_intv = pre_intv_logits.detach().argmax(dim=-1) \n",
    "    probe_trans.append(labels_pre_intv) # add the probe's translation to the list\n",
    "    '''后续layers'''\n",
    "    for i, layer in enumerate(range(layer_s, layer_e - 1)):  # 4, 5, 6, 7, indices of the layers to be passed\n",
    "        p = probes[layer+1]\n",
    "        whole_mid_act = models[layer_s].forward_2nd_stage(whole_mid_act, layer, layer+1)[0]\n",
    "        mid_act = whole_mid_act[0, -1] #? first time step? [512, ]\n",
    "        pre_intv_logits = p(mid_act[None, :])[0].squeeze(0)\n",
    "        labels_pre_intv = pre_intv_logits.detach().argmax(dim=-1)\n",
    "        probe_trans.append(labels_pre_intv)\n",
    "    assert len(probe_trans) == 8\n",
    "    for i, tran in enumerate(probe_trans):\n",
    "        match = torch.tensor(pre_intv_truth).to('cuda:0') == tran.to(torch.get_default_dtype())\n",
    "        err = torch.count_nonzero(~match).item()\n",
    "        if err:\n",
    "            # print(\"_____________________________________________________________\")\n",
    "            # print(\"sample_id: \", j, \", layer: \", i+1)\n",
    "            # print(\"mismatch numbers: \", err)\n",
    "            # print(\"truth: \\n\", torch.tensor(pre_intv_truth).to('cuda:0'))\n",
    "            # print(\"translation: \\n\", tran.to(torch.get_default_dtype()))\n",
    "            # print(\"match resut: \\n\", match)\n",
    "            err_rate += err\n",
    "\n",
    "print(\"overall error rate: \", (err_rate / (64 * 8 * len(dataset))) * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Verify $D(x)=D(x')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the correctness of OthelloGPT's decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load the decoder'''\n",
    "championship = False\n",
    "\n",
    "mconf = GPTConfig(61, 59, n_layer=8, n_head=8, n_embd=512) #vocab_size = 61\n",
    "\n",
    "# models = {}\n",
    "# for layer in range(layer_s, layer_e):\n",
    "model = GPTforProbeIA(mconf, probe_layer=-1) #probe_layer is the last layer\n",
    "# model = GPT(mconf)\n",
    "\n",
    "'''load the retrained model (no layer norm in the decoder)'''\n",
    "retrained_path = \"./ckpts/OthelloGPT-no_lrnorm/non_layernorm_gpt__at_20240810_100610.ckpt\"\n",
    "load_res = model.load_state_dict(torch.load(retrained_path))\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    model = model.to(device)\n",
    "_ = model.eval()\n",
    "    # models[layer] = model\n",
    "\n",
    "'''\n",
    "decoder:\n",
    "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
    "  (head): Linear(in_features=512, out_features=61, bias=False)\n",
    "'''\n",
    "# print(model)\n",
    "decoder = nn.Sequential(model.ln_f, model.head)\n",
    "device = torch.cuda.current_device()\n",
    "decoder = decoder.to(device)\n",
    "_ = decoder.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "'''load the probe dataset for the last layer'''\n",
    "path = \"./probe_epsilon_dataset.json\"\n",
    "with open(path, 'r') as file:\n",
    "    json_data = file.read()\n",
    "data = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-c644e319733e>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print('x shape: ', torch.tensor(x).shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  torch.Size([1, 512])\n",
      "y shape:  torch.Size([64])\n",
      "pre_intv_pred shape:  torch.Size([1, 61])\n",
      "pre_intv_pred shape:  torch.Size([1, 61])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c644e319733e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mpre_intv_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_intv_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpre_intv_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpre_intv_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_intv_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_intv_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pre_intv_pred shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_intv_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"
     ]
    }
   ],
   "source": [
    "samples = data['layer_8']\n",
    "\n",
    "x = samples[777]['x']\n",
    "x = torch.tensor(x).to(device)\n",
    "print('x shape: ', torch.tensor(x).shape)\n",
    "y = samples[777]['y']\n",
    "print('y shape: ', torch.tensor(y).shape)\n",
    "pre_intv_pred = decoder(x)\n",
    "print(\"pre_intv_pred shape: \", pre_intv_pred.shape)  #[1, 61]\n",
    "pre_intv_pred = pre_intv_pred.view(-1, pre_intv_pred.size(-1))\n",
    "print(\"pre_intv_pred shape: \", pre_intv_pred.shape)  #\n",
    "pre_intv_pred = torch.softmax(pre_intv_pred, dim=0)\n",
    "padding = torch.zeros(2).cuda()\n",
    "pre_intv_pred = torch.cat([pre_intv_pred[:27], padding, pre_intv_pred[27:33], padding, pre_intv_pred[33:]], dim=0) \n",
    "print(\"pre_intv_pred shape: \", pre_intv_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 65 into shape (8,8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-206d0618df6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m sns.heatmap(pre_intv_pred.detach().cpu().numpy().reshape(8, 8), vmin=0., vmax=vv, \n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ABCDEFGH\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             annot=True, fmt=\".2f\")\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 65 into shape (8,8)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Visualize the decoder's prediction'''\n",
    "fig=plt.figure(figsize=(10, 6), dpi= 80, facecolor='w', edgecolor='k')\n",
    "vv = 0.2\n",
    "sns.heatmap(pre_intv_pred.detach().cpu().numpy().reshape(8, 8), vmin=0., vmax=vv, \n",
    "            yticklabels=list(\"ABCDEFGH\"), xticklabels=list(range(1,9)), square=True, \n",
    "            annot=True, fmt=\".2f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
